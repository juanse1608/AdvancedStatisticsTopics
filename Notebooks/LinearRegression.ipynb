{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Librerías\" data-toc-modified-id=\"Librerías-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Librerías</a></span></li><li><span><a href=\"#Introducción\" data-toc-modified-id=\"Introducción-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introducción</a></span></li><li><span><a href=\"#Historia-y-Evolución\" data-toc-modified-id=\"Historia-y-Evolución-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Historia y Evolución</a></span><ul class=\"toc-item\"><li><span><a href=\"#Las-Bases:-Pearson-y-Galton\" data-toc-modified-id=\"Las-Bases:-Pearson-y-Galton-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Las Bases: Pearson y Galton</a></span></li><li><span><a href=\"#Evolución-y-Polivalencia\" data-toc-modified-id=\"Evolución-y-Polivalencia-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Evolución y Polivalencia</a></span></li></ul></li><li><span><a href=\"#Algoritmo\" data-toc-modified-id=\"Algoritmo-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Algoritmo</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plantemiento\" data-toc-modified-id=\"Plantemiento-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Plantemiento</a></span></li><li><span><a href=\"#Supuestos\" data-toc-modified-id=\"Supuestos-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Supuestos</a></span></li></ul></li><li><span><a href=\"#Ejemplos\" data-toc-modified-id=\"Ejemplos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Ejemplos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Librerías:-ScikitLearn\" data-toc-modified-id=\"Librerías:-ScikitLearn-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Librerías: ScikitLearn</a></span></li><li><span><a href=\"#Librerías:-StatsModels\" data-toc-modified-id=\"Librerías:-StatsModels-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Librerías: StatsModels</a></span></li></ul></li><li><span><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusiones</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal - Historia y Ejemplos\n",
    "\n",
    "En este notebook se pretende resumir de manera general la historia de la [Regresión Lineal](https://en.wikipedia.org/wiki/Linear_regression), además de plantear algunos ejemplos y discutir un poco su evolución en el tiempo.\n",
    "\n",
    "Este notebook hace parte (y es el comienzo) de la serie de notebooks y scripts del curso de __Tópicos de Estadística Avanzada (DS y ML)__ tomado en la Universiad Nacional de Colombia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Librerías \n",
    "\n",
    "En esta seccion se importan las librerías y modulos que se van a usar a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:33:35.441882Z",
     "start_time": "2021-03-14T04:33:34.922034Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Se importan modelos y funciones param mostrar imagenes\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "# Librerias para la regresión lineal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera general la regresión lineal es una aproximación a modelar la relación entre una variable de respuesta continua, escalar o real y un set de variables explicatorias. En particular, la relación que se intenta establecer es de tipo __lineal__, aunque la generalización de esta aproximación permite entender relaciones de tipo __no lineal__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historia y Evolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar es importante resaltar que la información que se resumirá a continuación se obtuvo en su mayoría de [Stanton J. (2017)](https://www.tandfonline.com/doi/pdf/10.1080/10691898.2001.11910537), el cual resume de manera clara y breve la historia de este algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Las Bases: Pearson y Galton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Muchos atribuyen el comienzo del coeficiente de correlación de Pearson a, precisamente, Karl Pearson; sin embargo fue Galton, primo de Charles Darwin y admirador de la genética, quien uso su imaginación para llevar a lo que se conocería como el __PPMC__ (Pearson Product Moment Correlation) y la __regresión lineal__.\n",
    "\n",
    "Galton, con Pearson como su colega investigador, desarrollaría las bases de lo que se conoce como __regresión lineal simple:__ la relación entre una varibles escalar de respuesta y una variable explicativa. Y todo para entender el _impacto hereditario_ que podían tener ciertas varibles en las próximas generaciones. En particular, Galton estaba estudiando estas relaciones hereditarias de una planta de guisante dulce, lo cual dice mucho del __tremendo__ impacto y la importancia de la biología en la estadística (muchos de los grandes algoritmos y conceptos estadísticos han venido de resolver preguntas de la biología) y, por lo tanto, el Machine Learning y la Ciencia de Datos. \n",
    "\n",
    "A continuación se pueden ver imágenes (obtenidas de la web) de Galton y Pearson respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:12:26.568849Z",
     "start_time": "2021-03-14T02:12:26.564721Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRANCIS GALTON\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imagen de Galton\n",
    "print('FRANCIS GALTON')\n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/a/ae/Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg\", \n",
    "      width=200,\n",
    "      height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:14:24.811965Z",
     "start_time": "2021-03-14T02:14:24.808089Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KARL PEARSON\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Karl_Pearson%2C_1912.jpg\" width=\"200\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imagen de Pearson\n",
    "print('KARL PEARSON')\n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/1/18/Karl_Pearson%2C_1912.jpg\",\n",
    "      width=200,\n",
    "      height=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Después de la muerte de Galton, Pearson describe en el cuarto tomo de la biografía de Galton el comienzo del concepto de la pendiente de la regresión. En 1875, Galton le dió a sus amigos paquetes de plantas de guisantes y los pesos de las plantas eran similares en un paquete pero diferentes entre paquetes. Los amigos de Galton cuidaron las plantas hasta su próxima generación y después se la devolvieron a este. Galton gráfico los pesos de las plantas madres contra los pesos de las plantas hijas (la siguiente generación) y se dió cuenta que la mediana de las plantas madres de un paquete y la mediana de sus hijas describían prácticamente una linea recta y así comenzó a definir lo que sería conocido como el coeficiente $r$ o $\\beta$ de la regresión simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"../Imagenes/Linear Regression/SimpleRegression.png \" alt=\"drawing\" width=\"400\" title='Ejemplo de Regresión Lineal Simple'/>\n",
    "<center>  <font size=\"1\"> Ejemplo de la regresión lineal simple y las primeras gráficas que llevaron a Galton y Pearson a entender el coeficiente de pendiente. </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Evolución y Polivalencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A lo largo de los años, y después de que se estableciera de manera muy clara las bases teóricas y programáticas de este algortimo (que se explicarán en la próxima sección) el algoritmo ha evolucionado y se han creado variantes de este mismo como:\n",
    "\n",
    "1. __PLS__ (Mínimos Cuadrados Parciales). \n",
    "\n",
    "2. __GLM__ (Modelos Lineales Generalizados).\n",
    "\n",
    "3. __Elastic Net__ (Modelos Lineales Regularizados). \n",
    "\n",
    "Además de que este algoritmo ha servido como base y forjado los cimientos de muchos de los algoritmos de Machine Learning que se usan hoy en día: como __PCA__ (Análisis de Componentes Principales), __SVM__ (Máquinas de Soporte Vectorial) y __NN__ (Redes Neuronales), entre otros. Por ejemplo, en cuanto a redes neuronales, si se piensa bien detrás de cada capa y neurona hay un modelo lineal previo a un cambio (activación) no lineal de la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plantemiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dado un conjunto de datos $\\{X_1, X_2, \\ldots, X_n\\}$ donde cada X_i representa un conjunto de $p$ variables explicativas $\\{X_{1i}, \\ldots, X_{pi}\\}$ y la variable de repsuesta (originalmente escalar) para cada observación $\\{Y_1, \\ldots, Y_n\\}$ se busca encontrar los impactos $\\{\\beta_1, \\ldots, \\beta_p\\}$ que expliquen la relación entre cada variable explicativa y la respuesta:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p + \\epsilon$$\n",
    "\n",
    "donde $\\epsilon$ representa el error de la estimación (la mayoría de supuestos para poder obtener conclusiones estadísticas de este algoritmo se basan en ciertas reglas que deben cumplir lo residuales, es decir los errores empíricos encontrados. De manera matricial:\n",
    "\n",
    "$$Y = X\\boldsymbol\\beta + \\boldsymbol\\varepsilon$$\n",
    "donde \n",
    "$$Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\:\\:\\:\\:\\:\\:\\text{y}\\:\\:\\:\\:\\:\\: X = \\begin{bmatrix} 1 &  X_{11} & \\cdots & X_{1p} \\\\\n",
    " 1 & X_{21} & \\cdots & X_{2p} \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " 1 & X_{n1} & \\cdots & X_{np}\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "La solución del algoritmo, siempre cuando $X$ sea una matriz de rango completo, es estimar el vector $\\beta=(\\beta_1, \\ldots, \\beta_n)^T$ de la siguiente forma:\n",
    "\n",
    "$$\\hat \\beta = (X^TX)^{-1}X^TY$$\n",
    "y por lo tanto \n",
    "$$Y \\approx \\hat \\beta X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Supuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Los principales de supuestos que se deben cumplir son, entre otros:\n",
    "\n",
    "1. Exogeneidad débil.\n",
    "2. Linearidad. \n",
    "3. Varianza constante o homocedasticidad. \n",
    "4. Independencia de los errores/residuales.\n",
    "5. Falta de perfecta multicolinealidad entre las variables explicativas.\n",
    "\n",
    "Para entender cada uno de los supuestos ir [aquí](https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions) y para entender como chequearlos de manera empírica ir [aquí](https://www.hackdeploy.com/assumptions-of-linear-regression-with-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se presentan dos ejemplos para el mismo conjunto de datos usando dos frameworks distintos:\n",
    "\n",
    "1. `ScikitLearn`: Una de las librerías más usada de python que contiene algoritmos de regresión, clasificación, clusterizacion, reducción dimensional y más. Mucho más cercana al ML actual más que con la estadística.\n",
    "\n",
    "2. `StatsModels`: Es una librería mucho más tradicional desde el punto de vista del estadístico o matemático (y muy familiar al lenguaje `R`) que contiene en su gran mayoría métodos estadísticos y mucho detalle en los resultados en cuando a las pruebas de hipótesis, p-valores y demás que se implementan en un método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:25:46.853447Z",
     "start_time": "2021-03-14T04:25:46.712350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS DIMENSIONES DE LOS DATOS DE LOS ITEMS SON: (14256, 50)\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de los datos: vamos a leer un conjunto de datos de items de mercado libre y a estimar \n",
    "# la variable ITEM_SOLD_QUANTITY en funcion de algunas variables numericas como SELLER_REP_RATING_POS (el\n",
    "# porcentaje de rating positivo del vendedor del item) y ITEM_PRICE (precio del item)\n",
    "productos = pd.read_csv('../Data/MercadoLibre/PRODUCTOS.csv')\n",
    "productos.columns = productos.columns.str.upper()\n",
    "productos = productos.loc[~productos['ITEM_PRICE'].isna()]\n",
    "print('LAS DIMENSIONES DE LOS DATOS DE LOS ITEMS SON: {}'.format(productos.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Librerías: ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:31:18.042578Z",
     "start_time": "2021-03-14T04:31:18.034104Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL R SQUARE DEL MODELO ES 0.015539559805684444\n",
      "LOS COEFICIENTES DE IMPACTO SON [-6.53545872e-09  1.81214200e+02]\n",
      "EL INTERCEPTO ES 3.9794008951690785\n",
      "\n",
      "PREDICCION PARA UN ITEM DE 30000 Y VENDEDOR CON RATING POSTIVO DEL 1.0%:\n",
      "185.0 CANTIDADES VENDIDAS\n"
     ]
    }
   ],
   "source": [
    "# Se construye matriz de predictores o variables explicativas\n",
    "explicativas = ['ITEM_PRICE', 'SELLER_REP_RATING_POS']\n",
    "X = np.array(productos[explicativas])\n",
    "Y = np.array(productos['ITEM_SOLD_QUANTITY'])\n",
    "sk_reg = LinearRegression().fit(X, Y)\n",
    "print('EL R SQUARE DEL MODELO ES {}'.format(sk_reg.score(X, Y)))\n",
    "print('LOS COEFICIENTES DE IMPACTO SON {}'.format(sk_reg.coef_))\n",
    "print('EL INTERCEPTO ES {}'.format(sk_reg.intercept_))\n",
    "precio = 30000 ; rating_pos = 1.0\n",
    "print('\\nPREDICCION PARA UN ITEM DE {} Y VENDEDOR CON RATING POSTIVO DEL {}%:'.format(precio, rating_pos))\n",
    "print(np.round(sk_reg.predict(np.array([[precio, rating_pos]]))[0]), 'CANTIDADES VENDIDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como se puede ver, con este algoritmo en SKLearn se puede obtener información como los $\\beta$, el intercepto, el $R^2$ y muy poco más. Para más información ir [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Librerías: StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:39:16.308947Z",
     "start_time": "2021-03-14T04:39:16.293244Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     ITEM_SOLD_QUANTITY   R-squared:                       0.016\n",
      "Model:                            OLS   Adj. R-squared:                  0.015\n",
      "Method:                 Least Squares   F-statistic:                     112.5\n",
      "Date:                Sat, 13 Mar 2021   Prob (F-statistic):           3.37e-49\n",
      "Time:                        23:39:16   Log-Likelihood:            -1.0512e+05\n",
      "No. Observations:               14256   AIC:                         2.103e+05\n",
      "Df Residuals:                   14253   BIC:                         2.103e+05\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "ITEM_PRICE            -6.535e-09   2.19e-08     -0.299      0.765   -4.94e-08    3.64e-08\n",
      "SELLER_REP_RATING_POS   181.2142     12.772     14.188      0.000     156.179     206.249\n",
      "INTERCEPT                 3.9794     11.741      0.339      0.735     -19.035      26.994\n",
      "==============================================================================\n",
      "Omnibus:                    22273.013   Durbin-Watson:                   1.500\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          9378105.163\n",
      "Skew:                          10.201   Prob(JB):                         0.00\n",
      "Kurtosis:                     126.983   Cond. No.                     8.28e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.28e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Fit and summarize OLS model\n",
    "productos['INTERCEPT'] = 1\n",
    "sm_reg = sm.OLS(exog=productos[explicativas + ['INTERCEPT']], endog=productos['ITEM_SOLD_QUANTITY'])\n",
    "resultados = sm_reg.fit()\n",
    "print(resultados.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:35:38.321867Z",
     "start_time": "2021-03-14T04:35:38.318837Z"
    },
    "hidden": true
   },
   "source": [
    "¡Espectacular! Con esta librería se puede obtener todo lo del anterior ejemplo y mucho más, todo lo referente a las pruebas de hipótesis e inclusive unas notas finales con insights sobre el modelo. Para más información ir [aquí](https://www.statsmodels.org/stable/regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Además se pueden generar muchos gráficos relacionados a la regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:46:23.697102Z",
     "start_time": "2021-03-14T04:46:23.695168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# fig = sm.graphics.plot_partregress_grid(resultados)\n",
    "# fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Para más información ir [aquí](https://www.statsmodels.org/stable/examples/notebooks/generated/regression_plots.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "La regresión lineal remonta sus orígenes conceptuales a finales del siglo XIX y se origina a travñes de Karl Pearson y Francis Galto quienes buscaban entender impactos o correlaciones de previas generaciones en la nuevas a través de experimentos con plantas, es decir que nace de responder preguntas relacionadas a la biología.\n",
    "\n",
    "Los supuestos del algoritmo son casi (o más) tan importantes como el mismo algoritmo y permiten interpretar de una manera muy potente, a nivel estadístico, los resultados que arroja la regresión (sea simple o múltiple). Este algoritmo, a pesar de uno de los primeros algoritmos de predicción, es muy usado hoy en día y es una herramienta conocida por todo econometrista o estadístico. \n",
    "\n",
    "Ha servido como el padre para una nueva generación de algoritmos de __Machine Learning__ surgidos en el siglo XX y XXI: las redes neuronales, las máquinas de soporte de vectorial, métodos de reducción dimensional tienen por detrás mucho de lo usado en este algoritmo. Además de que este mismo a evolucionado en el tiempo: los modelos lineales generalizados, los modelos de elastic net (LASSO y Ridge), entre otros, son variaciones de este algoritmo que han buscado sobreponerse a ciertos problemas o incovenientes que tiene el algoritmo de regresión lineal usual. \n",
    "\n",
    "Por lo tanto, es y debería ser un requerimiento para todo científico de datos o ingeniero de machine learning el conocer de manera las bases tanto técnicas como programáticas de este algoritmo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Final del Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
